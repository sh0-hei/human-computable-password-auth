# 人間計算可能なパスワードに対しての機械学習での突破プログラムの実験レポート

## このレポートの目的

- 村田さん、丸野さんが行った人間計算可能なパスワードに関する実験について手元で実行してみることで、結果に関しての理解を深める。また、人間計算可能なパスワード、および評価に使われる機械学習の仕組みについての理解を深める。
- 新たなモデルでの実験を行い、他のモデルで実行した場合の結果を比較する。

## 背景

### 人間計算可能なパスワードの概要

典型的な英数字や記号を用いたパスワードには、簡単なパスワードは辞書型攻撃で突破されやすく、複雑なパスワードは人間が覚えられないという欠点が存在する。また、使い回されることも多く、パスワードが一つのサイトから流出したことにより他のサイトにログインされるなどの被害が発生することも多い。これらの欠点を補うためにManuel Blumによって提案されたのが人間計算可能なパスワードである。人間計算可能なパスワードでは、人間が英数字や記号を暗記する代わりに、画像から数値へのマッピングを記憶する。認証を行う段階では人間が提示された画像を元にある関数に従い計算を行い、計算結果を用いて認証する。
既存研究では、関数が攻撃者に知られている場合に50枚程度の組み合わせであれば突破されるということがわかっている。

### 基本的な機械学習の概要

機械学習とは、脳を模倣した構造であるニューラルネットワークと呼ばれる構造を用いてデータの処理・予測を行う手法である。機械学習は複数の層とそれらの層のノードを接続する重みで構成される。機械学習の手法の例としては、多層パーセプトロン、RNN, CNNの他、大規模言語モデルで注目を浴びたTransformerなどが代表的である。これらモデルに対して特定のデータを学習させるために、損失関数と呼ばれる関数を用いてモデルの正しさの評価が行われる。損失関数で評価されるモデルの正しさを向上させるために、誤差逆伝播法と呼ばれる手法を用いてモデルに対して設定されるパラメーターの更新が行われる。誤差逆伝搬を用いて最適化する手法としてはSGD, Adam, AdaGradなどが有名であり、これらは最適化アルゴリズムと呼ばれる。
まとめると、機械学習では次のような手法で推論が行われる。

1. データを学習させるためのモデルが用意される。また、モデルに対して学習を行うための、目的変数と正解データをセットとしたデータが大量に用意される。
2. モデルに対して重みがランダムに割り振られる。
3. モデルに対して学習データを入力し、損失関数によってモデルの正しさの評価が行われる。
4. モデルの正しさの評価を向上させるために、誤差逆伝搬と最適化アルゴリズムを用いて重みが調整される。
5. 最後に、学習データとは別のデータを入力し、モデルが未知のデータにどれほど対応できるのか (= 汎化性能を持つか)を評価する。未知のデータにも対応出来るモデルでない場合は、学習データのみに対応するモデルとなり、この現象を過学習と呼ぶ。

過学習を起こすと、既存のデータのみにしか対応出来ないモデルとなり、機械学習を用いる意味が無い状態になってしまう。そのため、過学習は避ける必要がある。

#### 多層パーセプトロンの概要

多層パーセプトロンとは、機械学習の手法の中でも最も簡易的なものである。多層パーセプトロンでは、単純に入力層、中間層、出力層のそれぞれの層の間で行列演算f(x) = η(Ax+b)が行われ、この演算を繰り返すことで推論が実行される。また、ノードごとにシグモイド関数などでの計算が行われる。ここで中間層は入力層、出力層と違い内容が見えないため隠れ層と呼ばれることもあり、また複数の中間層が存在する場合もある。

#### 損失関数の概要

損失関数とは、機械学習モデルの正しさの評価を行うために使用される関数である。機械学習モデルが予測した結果と正解データの間の誤差を計算することで機械学習モデルの正しさを評価する。損失関数には、二乗和誤差、交差エントロピー誤差、絶対誤差などがある。機械学習における"学習"の目的は、この損失関数の値を小さくすることでモデルの予測の正確性を向上することである。

#### 最適化アルゴリズムの概要

損失関数では機械学習モデルが実際にどこまで正しいかの評価を行うことが可能であるが、損失関数を用いて正しさの評価をすることはできても損失関数のみでは正しい形に近づける調整をすることはできない。この調整を行うことが最適化アルゴリズムの役割である。最適化アルゴリズムでは、損失関数の値を小さくするために、損失関数の勾配を用いて重みを調整する。損失関数の値を小さくするために、損失関数の値が小さくなる方向への勾配を微分によって求めることで、パラメーターの調整が可能である。最適化アルゴリズムには、SGD, Adam, AdaGradなどが代表的である。

#### 逆誤差伝搬法(Backpropagation)の概要

逆誤差伝搬法とは、機械学習における最適化を計算するために使われる手法である。機械学習の重みに対する損失関数の勾配を求めることが必要であることは上述した。機械学習モデルには複数のパラメーターが存在し、お互いに計算する際に依存しているため、勾配を求めるのが容易ではない。その計算を行うために逆誤差伝搬法という手法が用いられる。特に計算グラフと呼ばれる手法を用いて計算順序を整理し、微分をかんたんに行う手法が一般的である。

### 機械学習の発展的なアルゴリズムについて

機械学習のモデルとして、上記では最も簡易的なモデルである多層パーセプトロンについての紹介を行った。ここで、発展的な手法としてCNN, RNNやRNNの発展であるLSTMが提案されており、既に様々な場面で活用されている。ここではこれら手法に関して簡単にまとめるとともに、それぞれの手法に関しての特徴の比較を行う。

#### CNN(Convolutional Neural Network)

CNNとは、畳み込みニューラルネットワークとも呼ばれる手法である。特徴として、画像認識や物体検知、音声認識といったタスクに適用されることが多いという点が挙げられる。入力層、中間層（隠れ層）、出力層から構成されるという点は多層パーセプトロンと同様である。CNNでは、畳み込み層とプーリング層が追加されている。畳み込み層では、画像の特徴を抽出するためにフィルター処理を用いて画像の特徴を抽出する。プーリング層では、畳み込み層で抽出された特徴を圧縮するために用いられる。CNNでは、畳み込み層とプーリング層を繰り返すことが特徴である。

#### RNN(Recurrent Neural Network)

RNNとは、再帰的ニューラルネットワークとも呼ばれる手法である。特徴として、時系列データの処理に向いているという点が挙げられる。RNNの特徴として、内部に以前の入力による状態を持つという特徴がある。RNNの問題点として、データ量が増えた場合に勾配消失問題が発生するという点が挙げられる。また、CNNによる画像認識をRNNを用いて連続的に行うことで動画に対する推論を実行するモデルも提案されており、GoogLeNetと呼ばれる。

#### LSTM(Long Short-Term Memory)

LSTMとは、長短期記憶とも呼ばれる手法である。これはRNNの一種であり、RNNにおいて指摘されている勾配消失問題を解決した手法である。LSTMの特徴として、中間層で情報を保存するために通常のノードではなくLSTM Blockと呼ばれる構造を活用することがあげられる。LSTM Blockは、入力ゲート、出力ゲート、忘却ゲートから構成されている。

#### GRU(Gated Recurrent Unit)

GRUとは、LSTMモデルを簡略化し、計算量を抑えたモデルである。このモデルの特徴として、LSTMの長所である勾配消失問題への対策は行いつつも、LSTMの課題であった計算量の大きさを抑えたという点が挙げられる。LSTMでは入力ゲート、出力ゲート、忘却ゲートの3つのゲートが存在するが、GRUでは代わりにリセットゲートと更新ゲートの2つのゲートが存在する。

#### Dropout

Dropoutとは、過学習が起きがちな機械学習において、過学習を押さえるための層である。Dropoutでは、学習時にランダムに一部のノードが無効化され、0を出力するようになる。これにより、ノード間の依存関係がなくなり、過学習を抑えることができる。

### 既存研究について

既存の村田さん、丸野さんの研究では、人間計算可能なパスワードについてそれぞれ多層パーセプトロン、LSTMにて関数を予測する攻撃が成立しうるかについての研究が行われている。多層パーセプトロン、LSTMの両方において、関数の予測を行った結果として学習データのみに特化したモデルとなる過学習を起こしており、予測程度が10%程度になるという結論が出ている。また、関数を複雑にすることでこの予測成功率はさらに低下することがわかっており、機械学習による攻撃が成立しないことがわかっている。

## 実験内容

今回の実験では、次のことを行う。

- 村田さん、丸野さんの論文の結果の再現と再評価
- 他のモデルでの実行結果の比較

### 実験環境

この下に記載されているコードは次の環境で実行された。
CPU: Apple M1 Max
RAM: 32GB
OS: macOS Sonoma 14.0
Python: 3.11.5
Keras: 2.14.0
Tensorflow: 2.14.0

## 実験1: 村田さんの多層パーセプトロンのプログラムの再現

この実験では、村田さんの多層パーセプトロンのプログラムを再現する。このプログラムは、人間計算可能なパスワードの関数を予測するために多層パーセプトロンを用いている。このプログラムを実行すると、仮想的な人間計算可能なパスワードの流出データが生成され、この生成されたデータの学習データ、テストデータへの分割が行われる。さらに、多層パーセプトロンによる学習と推論が行われて、実行終了後にテストデータと学習データに対する推論の正解率が表示される。

### プログラムコード

プログラムを現行バージョンのtensorflow, pythonで実行するために次の修正を加えて実行した。

- np.float クラスがdeprecatedとなっており、実行するとエラーで停止するため、警告に従いfloat型をnp.float型の代わりに用いるようにした。

```python
#モジュールの読み込み
from __future__ import print_function

import pandas as pd
from pandas import Series,DataFrame

from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import numpy as np

import matplotlib.pyplot as plt

import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop
from keras.optimizers import Adam
from keras.optimizers.legacy import SGD
from keras.layers import Flatten

import tensorflow as tf
import random
import csv
import os
import seaborn as sns

# import warnings
import warnings
# filter warnings
warnings.filterwarnings('ignore')

n=26
Sgm = np.random.randint(0,10,n)
# define the secret mapping sigma
print(Sgm)

# Generate example pairs and save into the examples_pairs.csv file

data_size=2000
with open("train_pairs.csv", "w") as csvfile:
  writer = csv.writer(csvfile)
  writer.writerow(["X0", "X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13", "Z"])

  #csvfile.close()
  for j in range(data_size):
    X = np.random.randint(0,n,14)
    S_X = np.zeros(14)
    for k in range(14):
      S_X[k] = Sgm[X[k]]
    mid = ( S_X[10] + S_X[11] ) % 10
    Z = ( S_X[12] + S_X[13] + S_X[int(mid)] ) %10
    example = np.append(X,Z)
    writer.writerows([example])
  csvfile.close()

train = pd.read_csv("train_pairs.csv")
train = train.sample(frac=1).reset_index(drop=True)
print(train.shape)
train

x = train.drop(labels = ["Z"],axis = 1)
y = train["Z"]

#説明変数・目的変数をそれぞれ訓練データ・テストデータに分割
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state =41)

#データの整形
x_train = x_train.astype(float)
x_test = x_test.astype(float)

y_train = keras.utils.to_categorical(y_train,10)
y_test = keras.utils.to_categorical(y_test,10)
x_train

#ニューラルネットワークの構築
sgd = SGD(lr=0.001)
model = Sequential()
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

#ニューラルネットワークの学習
history = model.fit(x_train,y_train,batch_size=16, epochs=512, verbose=1, validation_data=(x_test, y_test))

#グラフ
def plot_history(history):
  # print(history.history.keys())

  # 精度の履歴をプロット
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.xlabel('epoch')
  plt.ylabel('accuracy')
  plt.legend(['acc', 'val_acc'], loc='lower right')
  plt.show()

  # 損失の履歴をプロット
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.xlabel('epoch')
  plt.ylabel('loss')
  plt.legend(['loss', 'val_loss'], loc='lower right')
  plt.show()

plot_history(history)
```

### 結果

結果は図2.1のようになり、村田さんの論文で指摘されていたように、過学習が起こっている傾向があった。学習したデータに対する推論はできていたが、未知のデータであるテストデータに対する正解率は10%程度しかなく、攻撃が成立しないことが確認された。また、図2.2のように損失関数の値も学習データに対しては小さくなっているが、テストデータに対しては大きくなっていることがわかる。これは過学習が起きていることを示している。


![図2.1](https://github.com/ik-y/human-computable-auth/assets/91519064/53b0ee77-aeec-4144-bf10-9f492909a2ac)

図2.1

![図2.2](https://github.com/ik-y/human-computable-auth/assets/91519064/bd31c87e-2d6a-42f7-8f37-3cc845956048)

図2.2

## 実験2: 丸野さんのLSTMのプログラムの再現

この実験では、実験1同様にLSTMでの予測が不可能であるとした丸野さんの論文に対しての再現実験を行う。このプログラムの動作は実験1と同様であるが、学習と推論には多層パーセプトロンの代わりにLSTMを用いている。

### 実験2: プログラムコード

論文にあったプログラムコードについて、次の修正を加えた上で実行した。
- keras.np_utils クラスが最新版のライブラリでは使用不可となっていたため、同様の機能を持つ関数であるkeras.utils.to_categoricalを用いた。
- 最新の実装であるkeras.optimizers.RMSPropについて、実行時にM1/M2 CPUでは正常に実行できない警告が表示され途中で実行が停止していたため、警告にしたがい旧来の実装であるkeras.optimizers.legacy.RMSPropを用いた。

```python
#モジュールの読み込み
from __future__ import print_function
import pandas as pd
from pandas import Series,DataFrame
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, LSTM
from tensorflow.keras.optimizers.legacy import RMSprop
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import random
import numpy as np
import pandas as pd
import csv
import os
n=26
Sgm = np.random.randint(0,10,n)

# define the secret mapping sigma
print(Sgm)
# Generate example pairs and save into the examples_pairs.csv file
data_size=50000
with open("train_pairs.csv", "w") as csvfile:
  writer = csv.writer(csvfile)
  writer.writerow(["X0", "X1", "X2", "X3", "X4", "X5", "X6", "X7",
    "X8", "X9", "X10", "X11", "X12","X13", "Z"])
  for j in range(data_size):
    X = np.random.randint(0,10,14)#0～9 までの値を取るサイズ14 の配列
    mid=(X[10]+X[11])%10
    Z=(X[mid]+X[12])%10
    example = np.append(X,Z)
    writer.writerows([example])
  csvfile.close()

import seaborn as sns
import matplotlib.pyplot as plt
# import warnings
import warnings
# filter warnings
warnings.filterwarnings("ignore")
train = pd.read_csv("train_pairs.csv")
train = train.sample(frac=1).reset_index(drop=True)

x = train.drop(labels = ["Z"],axis = 1)
y = train["Z"]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

y_train =keras.utils.to_categorical(y_train,10)
y_test =keras.utils.to_categorical(y_test,10)
x_train = x_train.values
x_test = x_test.values
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)
x_train.shape
print(x_train)

model = Sequential()
model.add(LSTM(32, input_shape = (14, 1)))
model.add(Dense(10, activation="softmax"))
print("\n")

model.compile(loss="mean_squared_error", optimizer=RMSprop(), metrics=["accuracy"])
history = model.fit(x_train, y_train, batch_size=32, epochs=512, verbose=1, validation_data=(x_test, y_test))

score = model.evaluate(x_test,y_test,verbose=1)
print("\n")
print("Test loss:",score[0])
print("Test accuracy:",score[1])

def plot_history(history):
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.xlabel('epoch')
  plt.ylabel('accuracy')
  plt.legend(['acc', 'val_acc'], loc='lower right')
  plt.show()

  # 損失の履歴をプロット
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.xlabel('epoch')
  plt.ylabel('loss')
  plt.legend(['loss', 'val_loss'], loc='lower right')
  plt.show()

print(history.history.keys())
plot_history(history)

```

### 実験2: 結果

実験2の結果では、既存の丸野さんの論文と同様過学習が起こっている結果となった。また、損失関数の値も学習データに対しては小さくなっているが、テストデータに対しては大きくなっていることがわかる。これは過学習が起きていることを示している。

![図2.3](https://github.com/ik-y/human-computable-auth/assets/91519064/48e185c8-89b8-4e1e-8a8a-b7bef962b3b7)

図2.3

![図2.4](https://github.com/ik-y/human-computable-auth/assets/91519064/dc03aac1-2a47-476c-af54-9cdc162cbc59)

図2.4

## 実験3: GRUモデルでの実行結果の比較

この実験では、実験1, 2で行った実験について、他の機械学習モデルでも攻撃が不成立となるかの検証を行う。今回はLSTMモデルの簡略版であり計算量が小さい特徴があるGRUを用いて、代わりにバッチサイズを小さくして過学習を押さえることでどのような結果が出るかを検証する。

### 実験3: プログラムコード

```python
#モジュールの読み込み
from __future__ import print_function
import pandas as pd
from pandas import Series,DataFrame
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, GRU
from tensorflow.keras.optimizers.legacy import RMSprop
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import random
import numpy as np
import pandas as pd
import csv
import os
n=26
Sgm = np.random.randint(0,10,n)

# define the secret mapping sigma
print(Sgm)
# Generate example pairs and save into the examples_pairs.csv file
data_size=50000
with open("train_pairs.csv", "w") as csvfile:
  writer = csv.writer(csvfile)
  writer.writerow(["X0", "X1", "X2", "X3", "X4", "X5", "X6", "X7",
    "X8", "X9", "X10", "X11", "X12","X13", "Z"])
  for j in range(data_size):
    X = np.random.randint(0,10,14)#0～9 までの値を取るサイズ14 の配列
    mid=(X[10]+X[11])%10
    Z=(X[mid]+X[12])%10
    example = np.append(X,Z)
    writer.writerows([example])
  csvfile.close()

import seaborn as sns
import matplotlib.pyplot as plt
# import warnings
import warnings
# filter warnings
warnings.filterwarnings("ignore")
train = pd.read_csv("train_pairs.csv")
train = train.sample(frac=1).reset_index(drop=True)

x = train.drop(labels = ["Z"],axis = 1)
y = train["Z"]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

y_train =keras.utils.to_categorical(y_train,10)
y_test =keras.utils.to_categorical(y_test,10)
x_train = x_train.values
x_test = x_test.values
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)
x_train.shape
print(x_train)

model = Sequential()
model.add(GRU(32, input_shape = (14, 1)))
model.add(Dense(10, activation="softmax"))
print("\n")

model.compile(loss="mean_squared_error", optimizer=RMSprop(), metrics=["accuracy"])
history = model.fit(x_train, y_train, batch_size=32, epochs=512, verbose=1, validation_data=(x_test, y_test))

score = model.evaluate(x_test,y_test,verbose=1)
print("\n")
print("Test loss:",score[0])
print("Test accuracy:",score[1])

def plot_history(history):
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.xlabel('epoch')
  plt.ylabel('accuracy')
  plt.legend(['acc', 'val_acc'], loc='lower right')
  plt.show()

  # 損失の履歴をプロット
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.xlabel('epoch')
  plt.ylabel('loss')
  plt.legend(['loss', 'val_loss'], loc='lower right')
  plt.show()

print(history.history.keys())
plot_history(history)

```

### 実験3: 結果

GRUでの結果もLSTM同様の結果となり、過学習が顕著となった。また、損失関数の値も学習データに対しては小さくなっているが、テストデータに対しては大きくなっていることがわかる。これは過学習が起きていることを示している。

![図2.5](ttps://github.com/ik-y/human-computable-auth/assets/91519064/f2b25436-24a8-4784-88e1-5a9dcda3ef24)

図2.5

![図2.6](https://github.com/ik-y/human-computable-auth/assets/91519064/8a320257-7c18-4e2a-aa99-8195e5eeac99)

図2.6

## 実験4: GRU + Dropoutを用いた実験

実験1,2,3いずれも過学習が起こる傾向が顕著であった。そこで、過学習を抑制するDropoutを加えて実行した。

### 実験4: プログラムコード

```python
#モジュールの読み込み
from __future__ import print_function
import pandas as pd
from pandas import Series,DataFrame
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, GRU, Dropout
from tensorflow.keras.optimizers.legacy import RMSprop
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import random
import numpy as np
import pandas as pd
import csv
import os
n=26
Sgm = np.random.randint(0,10,n)

# define the secret mapping sigma
print(Sgm)
# Generate example pairs and save into the examples_pairs.csv file
data_size=50000
with open("train_pairs.csv", "w") as csvfile:
  writer = csv.writer(csvfile)
  writer.writerow(["X0", "X1", "X2", "X3", "X4", "X5", "X6", "X7",
    "X8", "X9", "X10", "X11", "X12","X13", "Z"])
  for j in range(data_size):
    X = np.random.randint(0,10,14)#0～9 までの値を取るサイズ14 の配列
    mid=(X[10]+X[11])%10
    Z=(X[mid]+X[12])%10
    example = np.append(X,Z)
    writer.writerows([example])
  csvfile.close()

import seaborn as sns
import matplotlib.pyplot as plt
# import warnings
import warnings
# filter warnings
warnings.filterwarnings("ignore")
train = pd.read_csv("train_pairs.csv")
train = train.sample(frac=1).reset_index(drop=True)

x = train.drop(labels = ["Z"],axis = 1)
y = train["Z"]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

y_train =keras.utils.to_categorical(y_train,10)
y_test =keras.utils.to_categorical(y_test,10)
x_train = x_train.values
x_test = x_test.values
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)
x_train.shape
print(x_train)

model = Sequential()
model.add(GRU(32, input_shape = (14, 1)))
model.add(Dense(10, activation="softmax"))
model.add(Dropout(0.2))
print("\n")

model.compile(loss="mean_squared_error", optimizer=RMSprop(), metrics=["accuracy"])
history = model.fit(x_train, y_train, batch_size=16, epochs=256, verbose=1, validation_data=(x_test, y_test))

score = model.evaluate(x_test,y_test,verbose=1)
print("\n")
print("Test loss:",score[0])
print("Test accuracy:",score[1])

def plot_history(history):
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.xlabel('epoch')
  plt.ylabel('accuracy')
  plt.legend(['acc', 'val_acc'], loc='lower right')
  plt.show()

  # 損失の履歴をプロット
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.xlabel('epoch')
  plt.ylabel('loss')
  plt.legend(['loss', 'val_loss'], loc='lower right')
  plt.show()

print(history.history.keys())
plot_history(history)

```

### 実験4: 結果

Dropoutを実装した結果は次の図2.6のようになり、なお過学習が起こっている傾向が観測された。これは学習データに対してこのモデルがそもそも適合できないということが推察される。よって、GRUモデルは人間計算可能なパスワードの予測に活用できないということがわかった。

![図2.7](https://github.com/ik-y/human-computable-auth/assets/91519064/c991e0fd-49bc-4171-8e4d-7f1b5f68746c)

図2.7

## 参考文献

- ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装、オライリージャパン 斎藤 康毅, 2016
- Mathworks, "畳み込みニューラル ネットワークとは", https://jp.mathworks.com/discovery/convolutional-neural-network-matlab.html
- Mathworks, "リカレントニューラルネットワーク (RNN) とは", https://jp.mathworks.com/discovery/rnn.html
- MathWorks, "googlenet", https://jp.mathworks.com/help/deeplearning/ref/googlenet.html
- Jeremiah Blocki, Manuel Blum, Anupam Datta, Santosh Vempala. Towards
Human Computable Psswords. Innovations in Theoretical Computer Science (ITCS)
