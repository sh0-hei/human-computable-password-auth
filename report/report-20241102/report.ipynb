{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMモデルに対して加えられる改善の調査レポート\n",
    "\n",
    "## 背景\n",
    "\n",
    "丸野さんの論文、及び以前の再現実験によって、LSTMが一定機械学習による人間計算可能なパスワードへの攻撃の制度を上昇させることがわかった。一方で、精度の向上幅はまだ限られており、攻撃が可能であるとは言い難い。そこで、今回分析しようとしている教師データが時系列データでないことなどに着目して、よりよい予測が可能になるのではないかと考えている。そのための調べものをしたのでまとめている。\n",
    "\n",
    "## LSTM/RNNの双方向化\n",
    "\n",
    "LSTMを含むRNNの手法は時系列データの解析に用いられる手法である。一方で、人間計算可能なパスワードは時系列に依存しないデータであり、LSTMの記憶機構は主に関数内の定数mappingの記憶に活用されていると考えられる。RNNは一般的には時系列に対して進行する方向にのみ記憶を行うが、双方向RNNと呼ばれる手法を用いることで双方向に記憶を保存することが可能である。双方向LSTMは次の要素から構成されている。\n",
    "\n",
    "- Forward LSTM\n",
    "- backward LSTM\n",
    "\n",
    "このうち、forward LSTMは通常のLSTMである。それに対して、backward LSTMひゃ通常のLSTMとは逆に時系列を遡る方向に記憶を行う学習器である。２つのLSTMをそれぞれ逆向きに使うことで双方向に記憶を伝播することが可能となる。\n",
    "つまり、双方向RNNは二つの方向のRNNを組み合わせた方向となっている。この方式でのRNNの活用例として、時系列データの補完などがあげられる。例えば動画でのフレーム間の画像の補完などが応用例である。\n",
    "\n",
    "## LSTM/RNNで隠れ状態の最終的なベクトルデータだけでなく学習途中でのデータを出力する\n",
    "\n",
    "RNNやLSTMでは一般的な手法として隠れ状態をベクトルデータとして内部に保存している。ここで隠れ状態として用いることができるベクトルは次元数がネットワーク構築時に決められており、次元数以上の要素の記憶を行うことが不可能である。そのため、隠れ状態のベクトルは一般的には最終的なベクトルのみが出力される。しかし、隠れ状態のベクトルは学習途中でのデータも含んでいるため、学習途中でのデータを出力することでより良い予測が可能になるのではないかと考えられる。RNNの隠れ状態のベクトルを時系列順にまとめたものをレイヤー間で渡して計算に用いることでより多くの記憶を保存することが可能である。\n",
    "\n",
    "## 実験内容\n",
    "\n",
    "今回は、これらの手法を用いた新たなLSTMモデルを用いた関数予測を検討してみる。ここで用いられる関数は以前のレポートで紹介された関数と同一のものである。ここで、次のモデルを試した。\n",
    "\n",
    "- 双方向lstm(bidirectional_lstm)\n",
    "- より深くした双方向lstm(deep_bidirectional_lstm)\n",
    "- より深くしたlstmにdropoutを付加して過学習を抑えたもの(depp_lstm_with_dropout)\n",
    "- 丸野さんの論文で提案されていたモデル(lstm_with_adam)\n",
    "- 村田さんの論文で提案されていたモデル(mlp)\n",
    "\n",
    "また、関数は次のものを試した。\n",
    "\n",
    "- 村田さんの論文で提案されたf sigma(middle)\n",
    "- 村田さんの論文で提案されたf(s_x)\n",
    "- 単純に足し算を行う関数(simple_add)\n",
    "\n",
    "以下に実験で用いたコードを示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from computable_password_generator import ComputablePasswordGenerator\n",
    "from utils import Utils\n",
    "from models import Models\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "iter = 0\n",
    "try:\n",
    "  os.mkdir(\"outputs/{}\".format(iter))\n",
    "except:\n",
    "  pass\n",
    "for model in Models.list_models():\n",
    "  print(\"Runnning model: {}\".format(model.name))\n",
    "  for generator in ComputablePasswordGenerator.list_generators():\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    tensorboard = TensorBoard(log_dir=\"logging/{}/\".format(iter) + generator.name + \"_\" + model.name, histogram_freq = 1)\n",
    "    try:\n",
    "      print(\"Testing: generator: {}, model: {}\".format(generator.name, model.name))\n",
    "      print(\"Figure name: {}\".format(generator.name + \"_\" + model.name))\n",
    "      generated_passwords = generator.generator(model.required_data_size)\n",
    "      x_train, x_test, y_train, y_test = Utils.split_to_train_and_valid(generated_passwords)\n",
    "      x_train = model.resharper(x_train)\n",
    "      print(x_train.shape)\n",
    "      x_test = model.resharper(x_test)\n",
    "      model.model.fit(x_train,y_train,batch_size=model.batch_size, epochs=model.epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[early_stopping, tensorboard])\n",
    "      history = model.model.history.history\n",
    "      Utils.plot_history(history, \"{}/\".format(iter) + generator.name + \"_\" + model.name)\n",
    "    except Exception as e:\n",
    "      print(\"Error: generator: {}, model: {}\".format(generator.name, model.name))\n",
    "      print(e)\n",
    "      continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ComputablePasswordGenerator:\n",
    "  # 人間計算可能なパスワードの流出データを模したデータを自動生成する関数群\n",
    "  # このクラスの関数を実行すると、人間計算可能なパスワードを模したデータが生成され、csvファイルとして保存される\n",
    "  # 外部のプログラムから呼び出すときは、from computable_password_generator import ComputablePasswordGenerator としてimportを行う\n",
    "\n",
    "  # この関数群の呼び出しには、int: データ数を引数として与える\n",
    "  # この関数群はpandas.DataFrameをreturnする\n",
    "  class Utils:\n",
    "    @staticmethod\n",
    "    def sgm(n :int) -> np.ndarray:\n",
    "      sgm = np.random.randint(0,10,n)\n",
    "      return sgm\n",
    "\n",
    "  class GeneratorWithMetadata:\n",
    "    def __init__(self, generator, name :str):\n",
    "      self.generator = generator\n",
    "      self.name = name\n",
    "\n",
    "  @staticmethod\n",
    "  def list_generators() -> list:\n",
    "    generators = []\n",
    "    generators.append(ComputablePasswordGenerator.GeneratorWithMetadata(ComputablePasswordGenerator.s_x, \"s_x\"))\n",
    "    generators.append(ComputablePasswordGenerator.GeneratorWithMetadata(ComputablePasswordGenerator.simple_pointer, \"simple_pointer\"))\n",
    "    generators.append(ComputablePasswordGenerator.GeneratorWithMetadata(ComputablePasswordGenerator.password_with_middle, \"middle\"))\n",
    "    generators.append(ComputablePasswordGenerator.GeneratorWithMetadata(ComputablePasswordGenerator.password_simple_add, \"simple_add\"))\n",
    "    return generators\n",
    "\n",
    "  @staticmethod\n",
    "  def password_simple_add(datasize :int) -> np.ndarray:\n",
    "    result = []\n",
    "    for row in range(datasize):\n",
    "      X = ComputablePasswordGenerator.Utils.sgm(14)\n",
    "      Z = (X[0] + X[1] + X[2]) % 10\n",
    "      row = np.append(X, Z)\n",
    "      result.append(row)\n",
    "    table_array = np.array(result)\n",
    "    return pd.DataFrame(table_array, columns=[\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\",\n",
    "      \"X8\", \"X9\", \"X10\", \"X11\", \"X12\",\"X13\", \"Z\"])\n",
    "\n",
    "  @staticmethod\n",
    "  def password_with_middle(datasize :int) -> np.ndarray:\n",
    "    result = []\n",
    "    for row in range(datasize):\n",
    "      X = ComputablePasswordGenerator.Utils.sgm(14)\n",
    "      mid = (X[10] + X[11]) % 10\n",
    "      Z = (X[mid] + X[12]) % 10\n",
    "      row = np.append(X, Z)\n",
    "      result.append(row)\n",
    "    table_array = np.array(result)\n",
    "    return pd.DataFrame(table_array, columns=[\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\",\n",
    "      \"X8\", \"X9\", \"X10\", \"X11\", \"X12\",\"X13\", \"Z\"])\n",
    "\n",
    "  @staticmethod\n",
    "  def simple_pointer(datasize :int) -> np.ndarray:\n",
    "    result = []\n",
    "    for row in range(datasize):\n",
    "      X = ComputablePasswordGenerator.Utils.sgm(14)\n",
    "      mid_1 = (X[10] + X[11]) % 10\n",
    "      mid_2 = (mid_1 + X[13]) % 10\n",
    "      mid_3 = (mid_2 + X[12]) % 10\n",
    "      Z = [mid_3] + X[12]\n",
    "      row = np.append(X, Z)\n",
    "      result.append(row)\n",
    "    table_array = np.array(result)\n",
    "    return pd.DataFrame(table_array, columns=[\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\",\n",
    "      \"X8\", \"X9\", \"X10\", \"X11\", \"X12\",\"X13\", \"Z\"])\n",
    "  \n",
    "  @staticmethod\n",
    "  def s_x(datasize :int) -> np.ndarray:\n",
    "    result = []\n",
    "    for row in range(datasize):\n",
    "      X = ComputablePasswordGenerator.Utils.sgm(14)\n",
    "      S_X = np.zeros(14)\n",
    "      for k in range(14):\n",
    "        sgm = ComputablePasswordGenerator.Utils.sgm(14)\n",
    "        S_X[k] = sgm[X[k]]\n",
    "      mid = ( S_X[10] + S_X[11] ) % 10\n",
    "      Z = ( S_X[12] + S_X[13] + S_X[int(mid)] ) %10\n",
    "      row = np.append(X, Z)\n",
    "      result.append(row)\n",
    "    table_array = np.array(result)\n",
    "    return pd.DataFrame(table_array, columns=[\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\",\n",
    "      \"X8\", \"X9\", \"X10\", \"X11\", \"X12\",\"X13\", \"Z\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Flatten, Bidirectional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Models:\n",
    "  class ModelWithMetadata:\n",
    "    def __init__(self, model, name :str, batch_size :int, epochs :int, required_data_size :int):\n",
    "      self.model = model\n",
    "      self.name = name\n",
    "      self.batch_size = batch_size\n",
    "      self.epochs = epochs\n",
    "      self.required_data_size = required_data_size\n",
    "\n",
    "    def resharper(self, df :pd.DataFrame) -> (np.ndarray):\n",
    "      if self.name.find(\"lstm\") != -1:\n",
    "        print(\"hoge\")\n",
    "        return df.to_numpy().reshape(df.shape[0], df.shape[1], 1)\n",
    "      return df\n",
    "\n",
    "  @staticmethod\n",
    "  def list_models() -> list:\n",
    "    models = []\n",
    "    #models.append(Models.ModelWithMetadata(Models.deep_bidirectional_sequential_lstm_with_dropout(), \"deep_bidirectional_lstm_with_dropout_online\", 1, 1024, 50000))\n",
    "    models.append(Models.ModelWithMetadata(Models.deep_bidirectional_sequential_lstm_with_dropout(), \"deep_bidirectional_lstm_with_dropout\", 32, 4096, 50000))\n",
    "    models.append(Models.ModelWithMetadata(Models.deep_bidirectional_sequential_lstm(), \"deep_bidirectional_lstm\", 32, 4096, 50000))\n",
    "    models.append(Models.ModelWithMetadata(Models.deep_lstm_with_dropout(), \"deep_lstm_with_dropout\", 32, 4096, 50000))\n",
    "    models.append(Models.ModelWithMetadata(Models.bidirectional_sequential_lstm(), \"bidirectional_lstm\", 32, 4096, 50000))\n",
    "    models.append(Models.ModelWithMetadata(Models.mlp_model(), \"mlp\", 16, 1024, 10000))\n",
    "    models.append(Models.ModelWithMetadata(Models.simple_lstm_with_AMSGrad(), \"simple_lstm\", 32, 1024, 50000))\n",
    "    models.append(Models.ModelWithMetadata(Models.simple_lstm_with_adam(), \"lstm_with_adam\", 32, 1024, 50000))\n",
    "    return models\n",
    "\n",
    "  # 人間計算可能なパスワードの予測に使うための機械学習モデル群\n",
    "  # これらの関数を呼び出すと、指定したSequentialモデルがreturnされる\n",
    "  @staticmethod\n",
    "  def mlp_model() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "  @staticmethod\n",
    "  def simple_lstm_with_AMSGrad() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape = (14, 1)))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "  @staticmethod\n",
    "  def simple_lstm_with_adam() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape = (14, 1)))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "  \n",
    "  @staticmethod\n",
    "  def deep_lstm_with_dropout() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape = (14, 1), return_sequences=True, dropout=0.2))\n",
    "    model.add(LSTM(32, return_sequences=True, dropout=0.2))\n",
    "    model.add(LSTM(32, dropout=0.2))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "  @staticmethod\n",
    "  def bidirectional_sequential_lstm() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True), input_shape = (14, 1),))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "  \n",
    "  @staticmethod\n",
    "  def deep_bidirectional_sequential_lstm() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True), input_shape = (14, 1), ))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "  \n",
    "  @staticmethod\n",
    "  def deep_bidirectional_sequential_lstm_with_dropout() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.2), input_shape = (14, 1)))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(32, dropout=0.2)))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Utils:\n",
    "  # グラフのプロットを行う関数\n",
    "  # keras.model.fit.historyおよび画像の保存名を引数として取る\n",
    "  @staticmethod\n",
    "  def plot_history(history :str, name :str) -> None:\n",
    "    # 学習時の学習データおよび正解データに対する正解率を表示する\n",
    "    pyplot.clf()\n",
    "    pyplot.plot(history['accuracy'])\n",
    "    pyplot.plot(history['val_accuracy'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.legend(['acc', 'val_acc'], loc='lower right')\n",
    "    pyplot.savefig(\"outputs/\" + name + '_accuracy.png')\n",
    "\n",
    "    # 学習時の学習データおよび正解データに対する損失関数の値を表示する\n",
    "    pyplot.clf()\n",
    "    pyplot.plot(history['loss'])\n",
    "    pyplot.plot(history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.legend(['loss', 'val_loss'], loc='lower right')\n",
    "    pyplot.savefig(\"outputs/\" + name + '_loss.png')\n",
    "    return None\n",
    "\n",
    "  @staticmethod\n",
    "  def split_to_train_and_valid(generated_passwords :pd.DataFrame) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    generated_passwords = generated_passwords.sample(frac=1).reset_index(drop=True)\n",
    "    x = generated_passwords.drop(labels = [\"Z\"],axis = 1)\n",
    "    y = generated_passwords[\"Z\"]\n",
    "\n",
    "    # 説明変数・目的変数をそれぞれ訓練データ・テストデータに分割\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    # データの型変換\n",
    "    #x_train = x_train.astype(float)\n",
    "    #x_test = x_test.astype(float)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験結果\n",
    "\n",
    "実験の結果は次のようになった。\n",
    "\n",
    "### middle関数の場合\n",
    "\n",
    "- bidirectional_lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![middle_bidirectional_lstm_accuracy](../outputs/0/middle_bidirectional_lstm_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![middle_bidirectional_lstm_loss](../outputs/0/middle_bidirectional_lstm_loss.png)\n",
    "\n",
    "- deep_bidirectional_lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![middle_deep_bidirectional_lstm_accuracy](../outputs/0/middle_deep_bidirectional_lstm_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![middle_deep_bidirectional_lstm_loss](../outputs/0/middle_deep_bidirectional_lstm_loss.png)\n",
    "\n",
    "- deep_lstm_with_dropoutの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![middle_deep_lstm_with_dropout_accuracy](../outputs/0/middle_deep_lstm_with_dropout_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![middle_deep_lstm_with_dropout_loss](../outputs/0/middle_deep_lstm_with_dropout_loss.png)\n",
    "\n",
    "- lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![middle_lstm_accuracy](../outputs/0/middle_lstm_with_adam_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![middle_lstm_loss](../outputs/0/middle_lstm_with_adam_loss.png)\n",
    "\n",
    "- mlpの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![middle_mlp_accuracy](../outputs/0/middle_mlp_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![middle_mlp_loss](../outputs/0/middle_mlp_loss.png)\n",
    "\n",
    "### s_x関数の場合\n",
    "\n",
    "- bidirectional_lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![s_x_bidirectional_lstm_accuracy](../outputs/0/s_x_bidirectional_lstm_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![s_x_bidirectional_lstm_loss](../outputs/0/s_x_bidirectional_lstm_loss.png)\n",
    "\n",
    "- deep_bidirectional_lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![s_x_deep_bidirectional_lstm_accuracy](../outputs/0/s_x_deep_bidirectional_lstm_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![s_x_deep_bidirectional_lstm_loss](../outputs/0/s_x_deep_bidirectional_lstm_loss.png)\n",
    "\n",
    "- deep_lstm_with_dropoutの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![s_x_deep_lstm_with_dropout_accuracy](../outputs/0/s_x_deep_lstm_with_dropout_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![s_x_deep_lstm_with_dropout_loss](../outputs/0/s_x_deep_lstm_with_dropout_loss.png)\n",
    "\n",
    "- lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![s_x_lstm_accuracy](../outputs/0/s_x_lstm_with_adam_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![s_x_lstm_loss](../outputs/0/s_x_lstm_with_adam_loss.png)\n",
    "\n",
    "- mlpの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![s_x_mlp_accuracy](../outputs/0/s_x_mlp_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![s_x_mlp_loss](../outputs/0/s_x_mlp_loss.png)\n",
    "\n",
    "### simple_add関数の場合\n",
    "\n",
    "- bidirectional_lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![simple_add_bidirectional_lstm_accuracy](../outputs/0/simple_add_bidirectional_lstm_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![simple_add_bidirectional_lstm_loss](../outputs/0/simple_add_bidirectional_lstm_loss.png)\n",
    "\n",
    "- deep_bidirectional_lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![simple_add_deep_bidirectional_lstm_accuracy](../outputs/0/simple_add_deep_bidirectional_lstm_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![simple_add_deep_bidirectional_lstm_loss](../outputs/0/simple_add_deep_bidirectional_lstm_loss.png)\n",
    "\n",
    "- deep_lstm_with_dropoutの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![simple_add_deep_lstm_with_dropout_accuracy](../outputs/0/simple_add_deep_lstm_with_dropout_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![simple_add_deep_lstm_with_dropout_loss](../outputs/0/simple_add_deep_lstm_with_dropout_loss.png)\n",
    "\n",
    "- lstmの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![simple_add_lstm_accuracy](../outputs/0/simple_add_lstm_with_adam_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![simple_add_lstm_loss](../outputs/0/simple_add_lstm_with_adam_loss.png)\n",
    "\n",
    "- mlpの場合\n",
    "\n",
    "精度\n",
    "\n",
    "![simple_add_mlp_accuracy](../outputs/0/simple_add_mlp_accuracy.png)\n",
    "\n",
    "損失関数\n",
    "\n",
    "![simple_add_mlp_loss](../outputs/0/simple_add_mlp_loss.png)\n",
    "\n",
    "\n",
    "## 考察\n",
    "\n",
    "双方向RNNを用いることで、狙い通りに村田さんが提案した関数に対する正解率が既存のRNNと比較して向上した。具体的な数値としては、既存の研究では13%ほどであった正解率が50%程度まで向上していることが見て取れる。一方で、まだ正解率が15%程度であることから、依然として機械学習を用いたセキュリティの突破は現実的ではない。また、単純な足し算関数に関しては村田さんの論文でも50%程度正解できるとの結果が出ていたが、本手法を用いることにより正解率をほぼ100%まで向上させることができた。今後は他のLSTM手法および複雑なモデルによる突破も検討する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
